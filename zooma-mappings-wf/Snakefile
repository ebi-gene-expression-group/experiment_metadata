from os import listdir
import re

# This workflow updates all ontology mappings in condensed SDRF files, and then
# updates all experiment designs on ves-hx-76:8080


# source bash functions

# need somewhere a mapping part, to go from many accessions build the unified reports (agreggation)


# Parse config from command line

mode = config.get("mode")


def get_accessions(mode):
    """
    Generate the expected output file names.
    Not yet sure if for all modes, the glob path below
    is where the accession dirs are found.
    Edit accordingly once known.
    """
    workdir = "" # Should be determined by mode
    acc_regex = re.compile("E-\D+-\d+")
    acc_dirs = listdir(f"{workdir}")
    ACCESSIONS = [acc for acc in acc_dirs if acc_regex.match(acc)]
    return(workdir, ACCESSIONS)

workdir, ACCESSIONS = get_accessions(mode)


# Below probably no longer needed bec of regex above
wildcard_constraints:
    accession = "E-\D+-\d+"


# Rule for running the whole pipelien
# Not yet sure if below is the desired path for the output condensed sdrf files
# Edit accordingly once the output dir is identified
rule all:
    input:
        condensed_sdrf=expand("{workdir}/{acc}/{acc}.condensed-sdrf.tsv", acc=ACCESSIONS),




# MEM_FOR_APPLY_FIXES=
# or adjust via retrial system


rule check_zooma:
    """
    Check that zooma returns successful http code
    """
    log:
        "logs/{accession}-check_zooma.log"
    output:
        temp("{accession}/check_zooma.done")
    params:
        zoomaMetadataUrl=config['zoomaMetadataUrl']
    shell:
        """
        set -e # snakemake on the cluster doesn't stop on error when --keep-going is set
        exec &> "{log}"

        httpResponse=`curl -o /dev/null -X GET -s -w %{http_code} {params.zoomaMetadataUrl}`
        if [ "$httpResponse" -e 200 ]; then
            touch {output}
        else
            echo "ERROR: Zooma not responding correctly"
            echo "{params.zoomaMetadataUrl} returned a non-success http code: $httpResponse for {wildcards.accession}"
        fi
        """

rule remove_aux_files:
    """
    Remove auxiliary files
    """
    log:
        "logs/{accession}-remove_aux_files.log"
    input:
        rules.check_zooma.output
    output:
        temp("{accession}/remove_aux_files.done")
    shell:
        """
        set -e # snakemake on the cluster doesn't stop on error when --keep-going is set
        exec &> "{log}"
        rm -rf {wildcards.accession}/condense_sdrf.???
        rm -rf {wildcards.accession}/fixes.???
        rm -rf {wildcards.accession}/{wildcards.accession}-zoomifications-log.tsv
        #rm -rf $zoomaMappingReport.aux
        touch {output}
        """


rule run_condense_sdrf:
    """
    Run condense_sdrf.pl with options to map terms with Zooma (-z) and import
    the IDF from ArrayExpress load directory (-i).
    """
    conda: "envs/perl-atlas-modules.yaml"
    log:
    input:
        "{accession}-configuration.xml"
    params:
        mode=config['mode']
    output:

    shell:
        """
        set -e # snakemake on the cluster doesn't stop on error when --keep-going is set
        exec &> "{log}"

        # Run condense_sdrf.pl with options to map terms with Zooma (-z) and import
        # the IDF from ArrayExpress load directory (-i).
        if [ {params.mode} == "atlas" ]; then

            expType=$(get_experiment_type_from_xml.pl {input})

            if [[ $expType == *baseline ]]; then
                ../condense_sdrf.pl ..
            else
                ../condense_sdrf.pl ..
            fi

        # if error detected, then condense SDRF without Zooma mapping:
        ..condense_sdrf.pl

        #if success, run next rule
        """


rule apply_fixes:
    """
    Apply automatic fixes to all imported sdrf and idf files
    """


rule check_zooma_mapping_report:


rule copy_reports_to_targetDir:
    """
    Copy reports to $targetDir.
    Mail out the Zoomification reports location
    """

