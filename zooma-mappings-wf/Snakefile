import re
from os import listdir
from datetime import datetime

# This workflow updates all ontology mappings in condensed SDRF files, and then
# updates all experiment designs on ves-hx-76:8080


# Parse config from command line

mode = config.get("mode")
working_dir = config.get("working_dir")


def get_accessions(working_dir):
    """
    Generate the expected output file names.
    Not yet sure if for all modes, the listdir path below
    is where the accession dirs are found.
    Edit accordingly once known.
    """
    acc_regex = re.compile("E-\D+-\d+")
    acc_dirs = listdir(f"{working_dir}")
    ACCESSIONS = [acc for acc in acc_dirs if acc_regex.match(acc)]
    return ACCESSIONS

ACCESSIONS = get_accessions(working_dir)


def zooma_mapping_report(wildcards):
    x = datetime.now()
    date_time = x.strftime("%Y-%m-%d")
    return f"{config['temp_dir']}/{wildcards['accession']}_{config['mode']}_refresh_experiment_metadata.{date_time}.log"

def get_attempt(wildcards, attempt):
    return attempt

# Below probably no longer needed bec of regex above
wildcard_constraints:
    accession = "E-\D+-\d+"


# Rule for running the whole pipeline
# Not yet sure if below is the desired path for the output condensed sdrf files
# Edit accordingly once the output dir is identified
rule all:
    input: 
        completed=expand(working_dir+"/{acc}/{acc}.-copy_reports_to_targetDir.done", acc=ACCESSIONS)
        #condensed_sdrf=expand(f"{working_dir}/{acc}/{acc}.condensed-sdrf.tsv", acc=ACCESSIONS),
        #apply_fixes=expand(f"{working_dir}/{acc}/{acc}.apply_fixes.done", acc=ACCESSIONS)

rule check_zooma:
    """
    Check that zooma returns successful http code
    """
    log:
        "{working_dir}/{accession}/logs/{accession}-check_zooma.log"
    output:
        temp("{working_dir}/{accession}/check_zooma.done")
    params:
        zoomaMetadataUrl=config['zoomaMetadataUrl']
    shell:
        """
        set -e # snakemake on the cluster doesn't stop on error when --keep-going is set
        exec &> "{log}"

        httpResponse=`curl -o /dev/null -X GET -s -w %{http_code} {params.zoomaMetadataUrl}`
        if [ "$httpResponse" -e 200 ]; then
            touch {output}
        else
            echo "ERROR: Zooma doesn't respond correctly"
            echo "{params.zoomaMetadataUrl} returned a non-success http code: $httpResponse for {wildcards.accession}"
        fi
        """

rule remove_aux_files:
    """
    Remove auxiliary files
    """
    log:
        "{working_dir}/{accession}/logs/{accession}-remove_aux_files.log"
    input:
        rules.check_zooma.output
    params:
        working_directory=working_dir,
        zoomaMappingReport=zooma_mapping_report
    output:
        temp("{working_dir}/{accession}/remove_aux_files.done")
    shell:
        """
        set -e # snakemake on the cluster doesn't stop on error when --keep-going is set
        exec &> "{log}"
        pushd {params.working_directory}/{wildcards.accession}
        rm -rf condense_sdrf.???
        rm -rf fixes.???
        rm -rf {wildcards.accession}-zoomifications-log.tsv
        rm -rf {params.zoomaMappingReport}.aux
        touch {output}
        # save a backup of the condensed SDRF file if present
        if [ -s "{wildcards.accession}.condensed-sdrf.tsv" ]; then
	        mv {params.working_directory}/{wildcards.accession}/{wildcards.accession}.condensed-sdrf.tsv {params.working_directory}/{wildcards.accession}/{wildcards.accession}.condensed-sdrf.tsv.bak
        fi
        popd
        """

rule run_condense_sdrf:
    """
    Run condense_sdrf.pl with options to map terms with Zooma (-z) and import
    the IDF from ArrayExpress load directory (-i).
    """
    conda: 
        "envs/perl-atlas-modules.yaml"
    log:
        "{working_dir}/{accession}/logs/{accession}-run_condense_sdrf.log"
    input:
        config_xml="{working_dir}/{accession}/{accession}-configuration.xml",
        rm_aux_done=rules.remove_aux_files.output,
        check_zooma_done=rules.check_zooma.output
    params:
        mode=config['mode'],
        attempt_number=get_attempt,
        retry_without_zooma=config['retryWithoutZooma'],
        script_dir=config['atlas_prod_co'],
        experiment_metadata_dir=config['experiment_metadata_dir'],
        working_directory=working_dir,
        zooma_exclusions=config['zooma_exclusions']
    output:
        "{working_dir}/{accession}/{accession}.condensed-sdrf.tsv"
    shell:
        """
        set -e # snakemake on the cluster doesn't stop on error when --keep-going is set
        exec &> "{log}"

        # For condense_sdrf.pl, get_experiment_type_from_xml.pl
        export PATH={params.script_dir}/db/scripts:$PATH
        # source bash functions
        source {params.script_dir}/bash_util/generic_routines.sh

        pushd {params.working_directory}/{wildcards.accession}

        if [[ {params.attempt_number} -lt 2 ]]; then
            # Run condense_sdrf.pl with options to map terms with Zooma (-z) and import
            # the IDF from ArrayExpress load directory (-i).
            if [ {params.mode} == "atlas" ]; then

                # Get the experiment type from the experiment config.
                expType=$(perl get_experiment_type_from_xml.pl {input.config_xml})

                if [[ $expType == *baseline ]]; then
                    {params.experiment_metadata_dir}/condense_sdrf.pl -e {wildcards.accession} -f {params.working_directory}/{wildcards.accession}/{wildcards.accession}-factors.xml -z -i -o {params.working_directory}/{wildcards.accession} -x {params.zooma_exclusions}
                else
                    {params.experiment_metadata_dir}/condense_sdrf.pl -e {wildcards.accession} -z -i -o {params.working_directory}/{wildcards.accession}
                fi
            elif [ "$mode" == "single_cell" ]; then
                export EXP_ID={wildcards.accession}
                {params.experiment_metadata_dir}/single_cell_condensed_sdrf.sh

            elif [ "$mode" == "irap_single_lib" ]; then
                # also collect biological replicate IDs for irap_single_lib mode
                {params.experiment_metadata_dir}/condense_sdrf.pl -e {wildcards.accession} -z -b -i -o {params.working_directory}/{wildcards.accession}
            else
                echo "Mode $mode not recognised."
                exit 1
            fi
        
        elif [[ {params.retry_without_zooma} == "yes" ]]; then

            # if error detected, then condense SDRF without Zooma mapping

            if [ {params.mode} == "atlas" ]; then

                expType=$(perl get_experiment_type_from_xml.pl {input})

                if [[ $expType == *baseline ]]; then
                    {params.experiment_metadata_dir}/condense_sdrf.pl -e {wildcards.accession} -f {params.working_directory}/{wildcards.accession}/{wildcards.accession}-factors.xml -i -o {params.working_directory}/{wildcards.accession} -x {params.zooma_exclusions}
                else
                    {params.experiment_metadata_dir}/condense_sdrf.pl -e {wildcards.accession} -i -o {params.working_directory}/{wildcards.accession}
                fi
            elif [ "$mode" == "single_cell" ]; then
                export EXP_ID={wildcards.accession}
                export SKIP_ZOOMA="yes" 
                {params.experiment_metadata_dir}/single_cell_condensed_sdrf.sh

            elif [ "$mode" == "irap_single_lib" ]; then
                # also collect biological replicate IDs for irap_single_lib mode
                {params.experiment_metadata_dir}/condense_sdrf.pl -e {wildcards.accession} -b -i -o {params.working_directory}/{wildcards.accession}
            else
                echo "Mode $mode not recognised."
                exit 1
            fi
        else
            echo "Failed to condense SDRF for {wildcards.accession}"
        fi

        # check if successful
        if [ ! -s "{params.working_directory}/{wildcards.accession}/{wildcards.accession}.condensed-sdrf.tsv" ]; then
	        echo "ERROR: Failed to generate {params.working_directory}/{wildcards.accession}/{wildcards.accession}.condensed-sdrf.tsv"
            rm -rf {params.working_directory}/{wildcards.accession}/{wildcards.accession}-zoomifications-log.tsv
	        exit 1
        else
            echo "All condense_sdrf tasks now done"
            echo "Updated condensed sdrf and idf files for all experiments"
        fi
        popd
        """


rule apply_fixes:
    """
    Apply automatic fixes to all imported sdrf and idf files.
    NOTE: We might need to add a submodule of atlas-bash-utils here
    """
    resources: 
        mem_mb=16000
    log:
        "{working_dir}/{accession}/logs/{accession}-apply_fixes.log"
    input:
        rules.run_condense_sdrf.output
        #autofix_properties="{exp_metadata_dir}/automatic_fixes_values.txt",
        #autofix_values="{exp_metadata_dir}/automatic_fixes_properties.txt",
        #idf="{working_dir}/{accession}/{accession}.idf.txt",
        #condensed_sdrf="{working_dir}/{accession}/{accession}.condensed-sdrf.tsv"
    params:
        working_directory=working_dir,
        script_dir=config['atlas_prod_co']
    output:
        temp("{working_dir}/{accession}/{accession}-apply_fixes.done")
    shell:
        """
        set -e # snakemake on the cluster doesn't stop on error when --keep-going is set
        exec &> "{log}"

        # source bash functions
        source {params.script_dir}/bash_util/generic_routines.sh

        echo "About to apply automatic fixes to all imported sdrf and idf files"

        pushd {params.working_directory}/{wildcards.accession}
        applyAllFixesForExperiment {wildcards.accession}
        touch {output}

        echo "All fixes tasks now done"
        popd
        touch {output}
        # we certainly don't want an email for each accession..
        # mailx -s 
        """

rule split_zooma_mapping_report:
    """
    Break down $zoomaMappingReport.aux into four report files: automatic, excluded, noresults, requirescuration
    """
    log:
        "{working_dir}/{accession}/logs/{accession}-check_zooma_mapping_report.log"
    input:
        zoomaMappingReport=zooma_mapping_report,
        fixes_done="{working_dir}/{accession}/{accession}-apply_fixes.done"
    params:
        split_report_files=expand(zooma_mapping_report+".aux.{section}.tsv", section=['AUTOMATIC','EXCLUDED','NO_RESULTS','REQUIRES_CURATION'])
    output:
        temp("{working_dir}/{accession}/{accession}-split_zooma_mapping_report.done")
    shell:
        """
        set -e # snakemake on the cluster doesn't stop on error when --keep-going is set
        exec &> "{log}"

        if [ ! -s "{input.zoomaMappingReport}.aux" ]; then
            echo "ERROR: Something went wrong with condense_sdrf.pl run with Zooma mappings - no report is available" 
            exit 1
        else
            # Now break down $zoomaMappingReport.aux into four report files: automatic, excluded, noresults, requirescuration
            source ./bin/split_zooma_mapping_report.sh

            split_zooma_mapping_report {params.zoomaMappingReport}

        fi

        for f in {params.split_report_files}; do
            if [ ! -s "$f"" ]; then
                echo "One of the split file not genetated for {input.zoomaMappingReport}"
                exit 1
            fi
        done

        touch {output}
        """
        

rule copy_reports_to_targetDir:
    """
    Copy reports to $targetDir.
    Mail out the Zoomification reports location
    """
    log:
        "{working_dir}/{accession}/logs/{accession}-copy_reports_to_targetDir.log"
    input:
        "{working_dir}/{accession}/{accession}-split_zooma_mapping_report.done"
    params:
        mode=config['mode'],
        working_directory=working_dir
    output:
        "{working_dir}/{accession}/{accession}-copy_reports_to_targetDir.done"
    shell:
        """
        set -e # snakemake on the cluster doesn't stop on error when --keep-going is set
        exec &> "{log}"
        today="`eval date +%Y-%m-%d`"

        # Copy reports to $targetDir
        if [ {params.mode} == "atlas" ]; then
            #targetDir=$ATLAS_FTP/curation/zoomage_reports/${today}
            targetDir={params.working_directory}/curation/zoomage_reports/${today}
        elif [ {params.mode} == "single_cell" ]; then
            #targetDir=$ATLAS_FTP/curation/zoomage_reports/single_cell/${today}
            targetDir={params.working_directory}/curation/zoomage_reports/single_cell/${today}
        elif [ {params.mode} == "irap_single_lib" ]; then
            #targetDir=$IRAP_SINGLE_LIB/zoomage/reports/${today}
            targetDir={params.working_directory}/zoomage/reports/${today}
        else
            echo "ERROR: mode: {params.mode} not recognised"
            exit 1
        fi
        mkdir -p $targetDir
        cp $zoomaMappingReport.AUTOMATIC.tsv $zoomaMappingReport.EXCLUDED.tsv $zoomaMappingReport.NO_RESULTS.tsv $zoomaMappingReport.REQUIRES_CURATION.tsv ${targetDir}/

        if [ ! -z ${previousRunDate+x} ] && [ -d ${targetDir}/../$previousRunDate ]; then
            # Calculating new lines not previously seen
            previousCurated=${targetDir}/../${previousRunDate}/atlas_zooma_mapping_report.${previousRunDate}.tsv.REQUIRES_CURATION.tsv
            newToCurate=${targetDir}/atlas_zooma_mapping_report.${today}.tsv.REQUIRES_CURATION.tsv
            # Compare based on fields Property value ($2), semantic tag ($3), Ontology label / Zooma mapping ($6)
            awk -F'\t' 'NR==FNR{e[$2$3$6]=1;next};!e[$2$3$6]' $previousCurated $newToCurate > ${targetDir}/atlas_zooma_mapping_report.${today}.tsv.REQUIRES_CURATION_NEW_LINES.tsv
        else
            echo "Variable previousRunDate value \"$previousRunDate\" does not take us to previous run, so no new lines" 
        fi

        # upon sucess, mail out the Zoomification reports location 
        echo -e "Dear curators,\n      Please find the Zooma mapping reports for the latest run for $today in $targetDir.\n\nGreetings from your friendly neighbourhood cron." | mutt -s "[${mode}/cron] Zooma mapping report is available in $targetDir" -- ${notifEmail}
        pushd $targetDir/../
        current_run=$(ls -ltr | grep -v previous_run | grep -e '[[:digit:]]\{4\}-[[:digit:]]\{2\}' | tail -n 1 | awk '{ print $9 }')
        rm -f previous_run
        ln -s $current_run previous_run
        popd
        # remove old condensed
        # rm -rf {wildcards.accession}.condensed-sdrf.tsv.bak
        touch {output}
        """



